verboseIter = FALSE)
# Define linear regression model
model <- train(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water +
Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
data = MVLRData,
method = "lm",
trControl = ctrl)
# Print cross-validation results
print(model)
# Access cross-validated performance metrics
cv_results <- model$results
library(caret)
# Load and preprocess data
MVLRData <- na.omit(trainData_clean)
# Define number of folds for cross-validation
num_folds <- 10  # You can adjust this number as needed
# Define control parameters for cross-validation
ctrl <- trainControl(method = "cv",
number = num_folds,
verboseIter = FALSE)
# Define linear regression model
model <- train(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water +
Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
data = MVLRData,
method = "lm",
trControl = ctrl)
# Print cross-validation results
print(model)
# Access cross-validated performance metrics
cv_results <- model$results
# Fit the model
model_lm <- lm(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water +
Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
data = MVLRData)
# Summarize the model
summary(model_lm)
# Predicted values
predicted <- predict(model_lm)
# Residuals
residuals <- resid(model_lm)
# Calculate Mean Squared Error (MSE)
mse <- mean(residuals^2)
# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)
## Calculate Mean Absolute Error (MAE)
mae <- mean(abs(residuals))
# Output MSE and RMSE
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
# Output MAE
cat("Mean Absolute Error (MAE):", mae, "\n")
library(caret)
# Load and preprocess data
MVLRData <- na.omit(trainData_clean)
# Define number of folds for cross-validation
num_folds <- 10  # You can adjust this number as needed
# Define control parameters for cross-validation
ctrl <- trainControl(method = "cv",
number = num_folds,
verboseIter = FALSE)
# Define linear regression model
model <- train(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water +
Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
data = MVLRData,
method = "lm",
trControl = ctrl)
# Print cross-validation results
print(model)
# Access cross-validated performance metrics
cv_results <- model$results
# Fit the model
model_lm <- lm(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water +
Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
data = MVLRData)
# Summarize the model
summary(model_lm)
# Predicted values
predicted <- predict(model_lm)
# Residuals
residuals <- resid(model_lm)
# Calculate Mean Squared Error (MSE)
mse <- mean(residuals^2)
# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)
## Calculate Mean Absolute Error (MAE)
mae <- mean(abs(residuals))
# Output MSE and RMSE
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
# Output MAE
cat("Mean Absolute Error (MAE):", mae, "\n")
# Preprocess the test dataset (if needed)
testData_clean <- na.omit(testData_clean)
# Extract the predictors from the test dataset
test_predictors <- testData_clean[, colnames(trainData_clean)[-which(names(trainData_clean) == "Strength")]]
# Predict the target variable using the linear regression model
predicted_test <- predict(model_lm, newdata = test_predictors)
# Actual values of the target variable from the test dataset
actual_values_test <- testData_clean$Strength
# Evaluate performance metrics on the test dataset
mse_test <- mean((predicted_test - actual_values_test)^2)
rmse_test <- sqrt(mse_test)
mae_test <- mean(abs(predicted_test - actual_values_test))
# Output performance metrics on the test dataset
cat("Test Set Mean Squared Error (MSE):", mse_test, "\n")
cat("Test Set Root Mean Squared Error (RMSE):", rmse_test, "\n")
cat("Test Set Mean Absolute Error (MAE):", mae_test, "\n")
install.packages("xgboost")
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Separate the predictors (features) and the target variable
X <- data[, -target_column_index]  # Replace 'target_column_index' with the index of the target column
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Separate the predictors (features) and the target variable
X <- data[, -target_column_index]  # Replace 'target_column_index' with the index of the target column
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age", "Strength")  # Replace with the names of your predictor columns
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Split the data into training and testing sets (e.g., 80% training and 20% testing)
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(data), 0.8 * nrow(data))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
# Train the XGBoost model
xgb_model <- xgboost(data = as.matrix(X_train), label = y_train, nrounds = 100, objective = "reg:squarederror")
# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X_test))
# Evaluate the model (e.g., using RMSE)
rmse <- sqrt(mean((predictions - y_test)^2))
print(paste("RMSE:", rmse))
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age", "Strength")  # Replace with the names of your predictor columns
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Set up k-fold cross-validation
nfolds <- 5  # Number of folds for cross-validation
# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")
# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$test_rmse_mean)
# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age")  # Exclude the target column
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Set up k-fold cross-validation
nfolds <- 5  # Number of folds for cross-validation
# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")
# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$evaluation_log$test_rmse_mean)
# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")
# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X))
# Evaluate the model (e.g., using RMSE)
rmse <- sqrt(mean((predictions - y)^2))
print(paste("RMSE:", rmse))
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age")  # Exclude the target column
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Set up k-fold cross-validation
nfolds <- 5  # Number of folds for cross-validation
# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")
# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$evaluation_log$test_rmse_mean)
# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")
# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X))
# Evaluate the model (e.g., using RMSE)
rmse <- sqrt(mean((predictions - y)^2))
print(paste("RMSE:", rmse))
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age")  # Exclude the target column
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Set up k-fold cross-validation
nfolds <- 5  # Number of folds for cross-validation
# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")
# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$evaluation_log$test_rmse_mean)
# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")
# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X))
# Calculate R-squared (R2)
r2 <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)
# Calculate adjusted R-squared (adjusted R2)
n <- nrow(X)
p <- ncol(X)
adjusted_r2 <- 1 - (1 - r2) * ((n - 1) / (n - p - 1))
# Calculate mean squared error (MSE)
mse <- mean((predictions - y)^2)
# Calculate mean absolute error (MAE)
mae <- mean(abs(predictions - y))
# Print the evaluation metrics
print(paste("RMSE:", rmse))
print(paste("R-squared (R2):", r2))
print(paste("Adjusted R-squared:", adjusted_r2))
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Mean Absolute Error (MAE):", mae))
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age")  # Exclude the target column
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Set up k-fold cross-validation
nfolds <- 10  # Number of folds for cross-validation
# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")
# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$evaluation_log$test_rmse_mean)
# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")
# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X))
# Calculate R-squared (R2)
r2 <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)
# Calculate adjusted R-squared (adjusted R2)
n <- nrow(X)
p <- ncol(X)
adjusted_r2 <- 1 - (1 - r2) * ((n - 1) / (n - p - 1))
# Calculate mean squared error (MSE)
mse <- mean((predictions - y)^2)
# Calculate mean absolute error (MAE)
mae <- mean(abs(predictions - y))
# Print the evaluation metrics
print(paste("RMSE:", rmse))
print(paste("R-squared (R2):", r2))
print(paste("Adjusted R-squared:", adjusted_r2))
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Mean Absolute Error (MAE):", mae))
#----------------------------EGB on test dataset--------------------
# Load the testData_clean dataset (assuming it's already loaded or imported)
# Replace 'testData_clean' with the actual name of your test dataset
test_data <- testData_clean
# Assuming you know the column names of your predictor variables in the test dataset
# If the test dataset includes the target variable, you may need to remove it
test_X <- test_data[, predictor_column_names]
# Make predictions on the test set
test_predictions <- predict(xgb_model, as.matrix(test_X))
# Calculate R-squared (R2) on the test set
test_r2 <- 1 - sum((test_data[, target_column_name] - test_predictions)^2) / sum((test_data[, target_column_name] - mean(test_data[, target_column_name]))^2)
# Calculate adjusted R-squared (adjusted R2) on the test set
test_n <- nrow(test_X)
test_p <- ncol(test_X)
test_adjusted_r2 <- 1 - (1 - test_r2) * ((test_n - 1) / (test_n - test_p - 1))
# Calculate mean squared error (MSE) on the test set
test_mse <- mean((test_predictions - test_data[, target_column_name])^2)
# Calculate mean absolute error (MAE) on the test set
test_mae <- mean(abs(test_predictions - test_data[, target_column_name]))
# Print the evaluation metrics for the test set
print(paste("Test R-squared (R2):", test_r2))
print(paste("Test Adjusted R-squared:", test_adjusted_r2))
print(paste("Test Mean Squared Error (MSE):", test_mse))
print(paste("Test Mean Absolute Error (MAE):", test_mae))
# Install and load necessary packages
library(xgboost)
# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean
# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
"Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age")  # Exclude the target column
# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]
# Set up k-fold cross-validation
nfolds <- 10  # Number of folds for cross-validation
# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")
# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$evaluation_log$test_rmse_mean)
# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")
# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X))
# Calculate R-squared (R2)
r2 <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)
# Calculate adjusted R-squared (adjusted R2)
n <- nrow(X)
p <- ncol(X)
adjusted_r2 <- 1 - (1 - r2) * ((n - 1) / (n - p - 1))
# Calculate mean squared error (MSE)
mse <- mean((predictions - y)^2)
# Calculate mean absolute error (MAE)
mae <- mean(abs(predictions - y))
# Print the evaluation metrics
print(paste("RMSE:", rmse))
print(paste("R-squared (R2):", r2))
print(paste("Adjusted R-squared:", adjusted_r2))
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Mean Absolute Error (MAE):", mae))
#----------------------------EGB on test dataset--------------------
# Load the testData_clean dataset (assuming it's already loaded or imported)
# Replace 'testData_clean' with the actual name of your test dataset
test_data <- testData_clean
# Assuming you know the column names of your predictor variables in the test dataset
# If the test dataset includes the target variable, you may need to remove it
test_X <- test_data[, predictor_column_names]
# Make predictions on the test set
test_predictions <- predict(xgb_model, as.matrix(test_X))
# Calculate R-squared (R2) on the test set
test_r2 <- 1 - sum((test_data[, target_column_name] - test_predictions)^2) / sum((test_data[, target_column_name] - mean(test_data[, target_column_name]))^2)
# Calculate adjusted R-squared (adjusted R2) on the test set
test_n <- nrow(test_X)
test_p <- ncol(test_X)
test_adjusted_r2 <- 1 - (1 - test_r2) * ((test_n - 1) / (test_n - test_p - 1))
# Calculate mean squared error (MSE) on the test set
test_mse <- mean((test_predictions - test_data[, target_column_name])^2)
# Calculate mean absolute error (MAE) on the test set
test_mae <- mean(abs(test_predictions - test_data[, target_column_name]))
# Print the evaluation metrics for the test set
print(paste("Test R-squared (R2):", test_r2))
print(paste("Test Adjusted R-squared:", test_adjusted_r2))
print(paste("Test Mean Squared Error (MSE):", test_mse))
print(paste("Test Mean Absolute Error (MAE):", test_mae))
#----------------------------EGB on test dataset--------------------
# Load necessary libraries
library(pROC)
# Load the testData_clean dataset (assuming it's already loaded or imported)
# Replace 'testData_clean' with the actual name of your test dataset
test_data <- testData_clean
# Assuming you know the column names of your predictor variables in the test dataset
# If the test dataset includes the target variable, you may need to remove it
test_X <- test_data[, predictor_column_names]
# Make predictions on the test set
test_predictions <- predict(xgb_model, as.matrix(test_X))
# Convert predicted values to binary (0 or 1) based on a threshold
threshold <- 0.5
predicted_binary <- ifelse(test_predictions >= threshold, 1, 0)
# Extract actual target values
actual <- test_data[, target_column_name]
# Calculate R-squared (R2) on the test set
test_r2 <- 1 - sum((actual - test_predictions)^2) / sum((actual - mean(actual))^2)
# Calculate adjusted R-squared (adjusted R2) on the test set
test_n <- nrow(test_X)
test_p <- ncol(test_X)
test_adjusted_r2 <- 1 - (1 - test_r2) * ((test_n - 1) / (test_n - test_p - 1))
# Calculate mean squared error (MSE) on the test set
test_mse <- mean((test_predictions - actual)^2)
# Calculate mean absolute error (MAE) on the test set
test_mae <- mean(abs(test_predictions - actual))
# Calculate accuracy
accuracy <- mean(predicted_binary == actual)
# Calculate sensitivity (True Positive Rate)
TP <- sum(predicted_binary == 1 & actual == 1)
FN <- sum(predicted_binary == 0 & actual == 1)
sensitivity <- TP / (TP + FN)
# Calculate specificity (True Negative Rate)
TN <- sum(predicted_binary == 0 & actual == 0)
FP <- sum(predicted_binary == 1 & actual == 0)
specificity <- TN / (TN + FP)
# Calculate Kappa
confusion_matrix <- table(actual, predicted_binary)
total_accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
expected_accuracy <- sum(rowSums(confusion_matrix) * colSums(confusion_matrix)) / sum(confusion_matrix)^2
kappa <- (total_accuracy - expected_accuracy) / (1 - expected_accuracy)
# Calculate AUC
roc_obj <- roc(actual, test_predictions)
auc <- auc(roc_obj)
# Print the evaluation metrics for the test set
print(paste("Test R-squared (R2):", test_r2))
print(paste("Test Adjusted R-squared:", test_adjusted_r2))
print(paste("Test Mean Squared Error (MSE):", test_mse))
print(paste("Test Mean Absolute Error (MAE):", test_mae))
print(paste("Accuracy:", accuracy))
print(paste("Sensitivity (True Positive Rate):", sensitivity))
print(paste("Specificity (True Negative Rate):", specificity))
print(paste("Kappa:", kappa))
print(paste("AUC (Area Under the Curve):", auc))
#----------------------------EGB on test dataset--------------------
# Load necessary libraries
library(pROC)
# Load the testData_clean dataset (assuming it's already loaded or imported)
# Replace 'testData_clean' with the actual name of your test dataset
test_data <- testData_clean
# Assuming you know the column names of your predictor variables in the test dataset
# If the test dataset includes the target variable, you may need to remove it
test_X <- test_data[, predictor_column_names]
# Make predictions on the test set
test_predictions <- predict(xgb_model, as.matrix(test_X))
# Convert predicted values to binary (0 or 1) based on a threshold
threshold <- 0.5
predicted_binary <- ifelse(test_predictions >= threshold, 1, 0)
# Extract actual target values
actual <- test_data[, target_column_name]
# Calculate R-squared (R2) on the test set
test_r2 <- 1 - sum((actual - test_predictions)^2) / sum((actual - mean(actual))^2)
# Calculate adjusted R-squared (adjusted R2) on the test set
test_n <- nrow(test_X)
test_p <- ncol(test_X)
test_adjusted_r2 <- 1 - (1 - test_r2) * ((test_n - 1) / (test_n - test_p - 1))
# Calculate mean squared error (MSE) on the test set
test_mse <- mean((test_predictions - actual)^2)
# Calculate mean absolute error (MAE) on the test set
test_mae <- mean(abs(test_predictions - actual))
# Calculate accuracy
accuracy <- mean(predicted_binary == actual)
# Calculate sensitivity (True Positive Rate)
TP <- sum(predicted_binary == 1 & actual == 1)
FN <- sum(predicted_binary == 0 & actual == 1)
sensitivity <- TP / (TP + FN)
# Calculate specificity (True Negative Rate)
TN <- sum(predicted_binary == 0 & actual == 0)
FP <- sum(predicted_binary == 1 & actual == 0)
specificity <- TN / (TN + FP)
# Calculate Kappa
confusion_matrix <- table(actual, predicted_binary)
total_accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
expected_accuracy <- sum(rowSums(confusion_matrix) * colSums(confusion_matrix)) / sum(confusion_matrix)^2
kappa <- (total_accuracy - expected_accuracy) / (1 - expected_accuracy)
# Calculate AUC
roc_obj <- roc(actual, test_predictions)
auc <- auc(roc_obj)
# Print the evaluation metrics for the test set
print(paste("Test R-squared (R2):", test_r2))
print(paste("Test Adjusted R-squared:", test_adjusted_r2))
print(paste("Test Mean Squared Error (MSE):", test_mse))
print(paste("Test Mean Absolute Error (MAE):", test_mae))
print(paste("Accuracy:", accuracy))
print(paste("Sensitivity (True Positive Rate):", sensitivity))
print(paste("Specificity (True Negative Rate):", specificity))
print(paste("Kappa:", kappa))
print(paste("AUC (Area Under the Curve):", auc))
# Check the unique values in the target variable
unique_classes <- unique(test_data[, Strength])
# Check the unique values in the target variable
unique_classes <- unique(test_data[, Strength])
# Check the unique values in the target variable
unique_classes <- unique(test_data[, Strength])
# Check the unique values in the target variable
unique_classes <- unique(testData_clean[, Strength])
View(testData_clean)
View(testData_clean)
colnames(testData_clean)
# Check the unique values in the target variable
unique_classes <- unique(testData_clean[, "Strength"])
# Print the unique classes
print(unique_classes)
# Determine the number of classes
num_classes <- length(unique_classes)
print(num_classes)
