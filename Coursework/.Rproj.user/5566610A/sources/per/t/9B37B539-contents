# Load the training dataset
train <- read.csv("concrete_strength_train.csv")

# Calculate relative frequency
train_freq <- prop.table(table(train$Strength))

# Summary statistics
summary_stats <- summary(train)
mean_value <- mean(train$Strength)
min_value <- min(train$Strength)
max_value <- max(train$Strength)
cov_value <- sd(train$Strength) / mean(train$Strength)  # Coefficient of variation

# Visual exploration of the Strength variable
library(ggplot2)
ggplot(train, aes(x = Strength, y = ..density..)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  geom_text(data = data.frame(x = c(0.8 * max(train$Strength)), y = c(0.04),
                              label = paste("Mean:", round(mean_value, 2),
                                            "\nMin:", min_value, "\nMax:",
                                            max_value, "\nCOV:", round(cov_value, 2))),
            aes(x = x, y = y, label = label),
            color = "red", hjust = 0, vjust = 1) +
  labs(title = "Distribution of Concrete Strength in Training Dataset",
       x = "Concrete Strength (MPa)",
       y = "Relative Frequency",
       caption = "Figure 1.1") +
  theme(plot.caption = element_text(hjust = 0))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Load the testing dataset
test <- read.csv("concrete_strength_test.csv")

# Calculate relative frequency
test_freq <- prop.table(table(test$Strength))

# Summary statistics
summary_stats <- summary(test)
mean_value <- mean(test$Strength)
min_value <- min(test$Strength)
max_value <- max(test$Strength)
cov_value <- sd(test$Strength) / mean(test$Strength)  # Coefficient of variation

# Visual exploration of the Strength variable
ggplot(test, aes(x = Strength, y = ..density..)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  geom_text(data = data.frame(x = c(0.8 * max(test$Strength)), y = c(0.04),
                              label = paste("Mean:", round(mean_value, 2),
                                            "\nMin:", min_value, "\nMax:",
                                            max_value, "\nCOV:", round(cov_value, 2))),
            aes(x = x, y = y, label = label),
            color = "red", hjust = 0, vjust = 1) +
  labs(title = "Distribution of Concrete Strength in Testing Dataset",
       x = "Concrete Strength (MPa)",
       y = "Relative Frequency",
       caption = "Figure 1.1") +
  theme(plot.caption = element_text(hjust = 0))

#-------------------------------------------Calculating missingness------------------------------
# Load required libraries
library(readr)
library(mice)

# Load train and test datasets
train_data <- read_csv("concrete_strength_train.csv")
test_data <- read_csv("concrete_strength_test.csv")

# Function to handle missing values
handle_missing_values <- function(data) {
  # Examine missingness by column
  col_missingness <- colMeans(is.na(data))
  
  # Calculate average missingness
  avg_missingness <- mean(col_missingness) * 100  # Convert to percentage
  
  # Print average missingness
  cat("Average missingness across columns:", avg_missingness, "%\n")
  
  if (avg_missingness > 1) {
    # Impute missing values using mice
    cat("Imputing missing values using mice()...\n")
    imputed_data <- mice(data)
    return(imputed_data)
  } else {
    # Remove missing values
    cat("Removing missing values...\n")
    clean_data <- na.omit(data)
    return(clean_data)
  }
}

# Apply the function to train and test datasets
clean_train_data <- handle_missing_values(train_data)
clean_test_data <- handle_missing_values(test_data)

#---------------------------------------PCA Cluster-----------------------------
# Load required libraries
library(caret)
library(readr)
library(dplyr)
library(FactoMineR)
library(ggplot2)

# Load train and test datasets
train_data <- read_csv("concrete_strength_train.csv")
test_data <- read_csv("concrete_strength_test.csv")

# Combine train and test datasets
all_data <- bind_rows(train_data, test_data)

# Separate features and target
X_all <- all_data %>% select(-`Strength`)

# Standardize the features
preprocess <- preProcess(X_all, method = c("center", "scale"))
X_all_scaled <- predict(preprocess, X_all)

# Perform PCA
pca_result <- PCA(X_all_scaled)

# Extract PCA scores
pca_scores <- pca_result$ind$coord

# Perform clustering 
num_clusters <- 3  # Number of clusters
set.seed(123)  # For reproducibility
clusters <- kmeans(pca_scores, centers = num_clusters)

# Convert PCA scores matrix to data frame
pca_scores_df <- as.data.frame(pca_scores)
names(pca_scores_df) <- c("Dim.1", "Dim.2")

# Add cluster information to PCA scores data frame
pca_scores_df$Cluster <- as.factor(clusters$cluster)

# Calculate cluster centroids
cluster_centroids <- clusters$centers

# Convert cluster centroids to data frame and add Cluster column
cluster_centroids_df <- as.data.frame(cluster_centroids)
cluster_centroids_df$Cluster <- as.factor(1:num_clusters)  # Add Cluster column

# Plot PCA scores with clusters and centroids
ggplot() +
  geom_point(data = pca_scores_df, aes(x = Dim.1, y = Dim.2, color = Cluster),
             shape = 21, size = 3) +
  geom_point(data = cluster_centroids_df, aes(x = Dim.1, y = Dim.2, fill = Cluster),
             shape = 19, color = "black", size = 4) +
  labs(x = "PC1",
       y = "PC2",
       title = "PCA Scores Plot with Clusters and Centroids") +
  scale_fill_manual(values = c("red", "green", "blue")) +
  scale_color_manual(values = c("red", "green", "blue")) +
  theme_minimal()

#-------------------------------------Removing outliers-------------------------
# Load your dataset
trainData <- read.csv("concrete_strength_test.csv")
testData <- read.csv("concrete_strength_train.csv")

# Function to identify outliers based on z-scores for each column
identify_outliers <- function(column, threshold) {
  z_scores <- scale(column)  # Calculate z-scores
  outliers <- abs(z_scores) > threshold  # Identify outliers
  return(outliers)
}

# Set a threshold for identifying outliers (e.g., z-score > 2 or z-score > 3)
threshold <- 2

# Initialize a list to store outlier information for each column
outliers_list <- list()

# Loop through each column (excluding non-numeric columns)
for (col in names(trainData)) {
  if (is.numeric(trainData[[col]])) {
    outliers <- identify_outliers(trainData[[col]], threshold)  # Identify outliers for the column
    outliers_list[[col]] <- outliers  # Store outlier information in the list
  }
}

# Combine outlier information for all columns
combined_outliers <- Reduce("|", outliers_list)

# Remove outliers from the dataset
trainData_clean <- trainData[!combined_outliers, ]

# Print the cleaned dataset
print(trainData_clean)

# Loop through each column (excluding non-numeric columns)
for (col in names(testData)) {
  if (is.numeric(testData[[col]])) {
    outliers <- identify_outliers(testData[[col]], threshold)  # Identify outliers for the column
    outliers_list[[col]] <- outliers  # Store outlier information in the list
  }
}

# Combine outlier information for all columns
combined_outliers <- Reduce("|", outliers_list)

# Remove outliers from the dataset
testData_clean <- testData[!combined_outliers, ]
# Print the cleaned dataset
print(testData_clean)
#------------------------------Correlation matrix-------------------------------
# Select only predictor variables (exclude the target variable)
predictor_vars <- subset(trainData_clean, select = -c(Strength))

# Compute the correlation matrix
correlation_matrix <- cor(predictor_vars)

# Print the correlation matrix
print(correlation_matrix)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~PART 2~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#------------------------------Multi-Variable Linear Regression-----------------
# Load necessary library
library(caret)

# Load and preprocess data
MVLRData <- na.omit(trainData_clean)

# Define number of folds for cross-validation
num_folds <- 10  # You can adjust this number as needed

# Define control parameters for cross-validation
ctrl <- trainControl(method = "cv", 
                     number = num_folds, 
                     verboseIter = FALSE)

# Define linear regression model
model <- train(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water + 
                 Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
               data = MVLRData,
               method = "lm",
               trControl = ctrl)

# Print cross-validation results
print(model)

# Access cross-validated performance metrics
cv_results <- model$results

# Fit the model
model_lm <- lm(Strength ~ Cement + Blast.Furnace.Slag + Fly.Ash + Water + 
                 Superplasticizer + Coarse.Aggregate + Fine.Aggregate + Age,
               data = MVLRData)

# Summarize the model
summary(model_lm)

# Predicted values
predicted <- predict(model_lm)

# Residuals
residuals <- resid(model_lm)

# Calculate Mean Squared Error (MSE)
mse <- mean(residuals^2)

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)

## Calculate Mean Absolute Error (MAE)
mae <- mean(abs(residuals))

# Output MSE and RMSE
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
# Output MAE
cat("Mean Absolute Error (MAE):", mae, "\n")

#--------------Multi-Variable Linear Regression model---------------------------
# Preprocess the test dataset (if needed)
testData_clean <- na.omit(testData_clean)

# Extract the predictors from the test dataset
test_predictors <- testData_clean[, colnames(trainData_clean)[-which(names(trainData_clean) == "Strength")]]

# Predict the target variable using the linear regression model
predicted_test <- predict(model_lm, newdata = test_predictors)

# Actual values of the target variable from the test dataset
actual_values_test <- testData_clean$Strength

# Evaluate performance metrics on the test dataset
mse_test <- mean((predicted_test - actual_values_test)^2)
rmse_test <- sqrt(mse_test)
mae_test <- mean(abs(predicted_test - actual_values_test))

# Output performance metrics on the test dataset
cat("Test Set Mean Squared Error (MSE):", mse_test, "\n")
cat("Test Set Root Mean Squared Error (RMSE):", rmse_test, "\n")
cat("Test Set Mean Absolute Error (MAE):", mae_test, "\n")

#---------------------------Decision Tree Model---------------------------------
library(rpart)

# Assuming the target variable is called "Strength"
trainData_clean <- na.omit(trainData_clean)
predictors <- trainData_clean[, -which(names(trainData_clean) == "Strength")]
target <- trainData_clean$Strength

# Train the decision tree model for regression
tree_model <- rpart(Strength ~ ., data = trainData_clean, method = "anova")

# Plot the decision tree
plot(tree_model)
text(tree_model)

# Print cross-validation results
cv_results <- printcp(tree_model)
print(cv_results)

# Set up cross-validation
cv <- rpart.control(cp = 0.01)  # Set the complexity parameter for cross-validation

# Perform cross-validation
cv_model <- rpart(Strength ~ ., data = trainData_clean, method = "anova", control = cv)

# Print cross-validation results
print(cv_model)

# Plot cross-validation results
plotcp(cv_model)

# Get the CP values from the cross-validation results
cp_values <- cv_model$cptable[, "CP"]

# Prune the tree at the 6th dot
cp_value <- cp_values[6]  

# Prune the tree using the selected CP value
pruned_tree <- prune(cv_model, cp = cp_value)

# Plot the pruned decision tree
plot(pruned_tree)
text(pruned_tree)

#------------------------Decision Tree on Test data-----------------------------
# Predict using the original tree_model
testData_clean <- na.omit(testData_clean)
predictions <- predict(tree_model, newdata = testData_clean)

# If you want to evaluate the performance of your model on the test data, you might want to compare predictions with the actual values if available in your test data
actual_values <- testData_clean$Strength

# Convert actual_values to numeric
actual_values <- as.numeric(as.character(actual_values))

# Assuming you have actual values, you can calculate some evaluation metrics such as RMSE or MAE
rmse <- sqrt(mean((predictions - actual_values)^2))
mae <- mean(abs(predictions - actual_values))

# If you want to compare it with the pruned tree, you can also predict using the pruned tree
pruned_predictions <- predict(pruned_tree, newdata = testData_clean)

# Convert pruned_predictions to numeric
pruned_predictions <- as.numeric(as.character(pruned_predictions))

pruned_rmse <- sqrt(mean((pruned_predictions - actual_values)^2))
pruned_mae <- mean(abs(pruned_predictions - actual_values))

# Print out the evaluation metrics
cat("Original Tree RMSE:", rmse, "\n")
cat("Original Tree MAE:", mae, "\n")

cat("Pruned Tree RMSE:", pruned_rmse, "\n")
cat("Pruned Tree MAE:", pruned_mae, "\n")

# Convert predictions and actual values to factors for binary classification
predictions_factor <- factor(ifelse(predictions >= 0.5, "Medium", "Low"), levels = c("Low", "Medium", "High")) # Adjust threshold as needed
actual_values_factor <- factor(ifelse(actual_values >= 0.5, "Medium", "Low"), levels = c("Low", "Medium", "High")) # Adjust threshold as needed

# Confusion matrix
conf_mat <- table(actual_values_factor, predictions_factor)

# Calculate Accuracy
accuracy <- sum(diag(conf_mat)) / sum(conf_mat)

# Calculate Sensitivity and Specificity
sensitivity <- ifelse(sum(conf_mat["Medium", ]) == 0, NaN, conf_mat["Medium", "Medium"] / sum(conf_mat["Medium", ]))
specificity <- ifelse(sum(conf_mat["Low", ]) == 0, NaN, conf_mat["Low", "Low"] / sum(conf_mat["Low", ]))

# Calculate False Positives (FP) and False Negatives (FN)
FP <- sum(conf_mat["Low", "Medium"]) + sum(conf_mat["Low", "High"])
FN <- sum(conf_mat["Medium", "Low"]) + sum(conf_mat["High", "Low"])

# Calculate Kappa
total_obs <- sum(conf_mat)
expected_accuracy <- sum(rowSums(conf_mat) * colSums(conf_mat)) / (total_obs^2)
kappa <- ifelse(expected_accuracy == 1, NaN, (accuracy - expected_accuracy) / (1 - expected_accuracy))

# Print results
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("False Positives (FP):", FP, "\n")
cat("False Negatives (FN):", FN, "\n")
cat("Kappa:", kappa, "\n")

# Assuming you have predictions and actual values
library(pROC)

# Create a ROC curve object
roc_curve <- roc(actual_values, predictions)

# Calculate AUC
auc_value <- auc(roc_curve)

# Print AUC
print(auc_value)

#-----------------------------Random Forest Model Training----------------------
library(randomForest)
library(caret)

# Define the number of folds for cross-validation
k <- 20

# Create a data frame containing both predictors and target variable
data <- trainData_clean[, c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
                            "Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
                            "Age", "Strength")]

# Perform k-fold cross-validation
folds <- createFolds(data$Strength, k = k, list = TRUE, returnTrain = FALSE)

# Create empty vectors to store evaluation metrics
mae <- numeric(k)
mse <- numeric(k)
rmse <- numeric(k)
r_squared <- numeric(k)

# Perform k-fold cross-validation
for (i in 1:k) {
  # Get the indices of the validation set for the current fold
  validation_indices <- folds[[i]]
  
  # Split the data into training and validation sets for the current fold
  train_set <- data[-validation_indices, ]
  validation_set <- data[validation_indices, ]
  
  # Train the random forest model using the training set
  rf_model <- randomForest(Strength ~ ., data = train_set, ntree = 100)
  
  # Make predictions on the validation set
  predictions <- predict(rf_model, newdata = validation_set)
  
  # Calculate evaluation metrics for the current fold
  mae[i] <- mean(abs(predictions - validation_set$Strength))
  mse[i] <- mean((predictions - validation_set$Strength)^2)
  rmse[i] <- sqrt(mse[i])
  r_squared[i] <- 1 - sum((predictions - validation_set$Strength)^2) / sum((validation_set$Strength - mean(validation_set$Strength))^2)
}

# Compute the average of evaluation metrics across all folds
avg_mae <- mean(mae)
avg_mse <- mean(mse)
avg_rmse <- mean(rmse)
avg_r_squared <- mean(r_squared)

# Print the average evaluation metrics
cat("Average Mean Absolute Error (MAE) across", k, "folds:", avg_mae, "\n")
cat("Average Mean Squared Error (MSE) across", k, "folds:", avg_mse, "\n")
cat("Average Root Mean Squared Error (RMSE) across", k, "folds:", avg_rmse, "\n")
cat("Average R-squared (R2) value across", k, "folds:", avg_r_squared, "\n")
#Plot QQ graph
qqnorm(residuals)
qqline(residuals, col = "red")

#-----------------------Random Forest on Test dataset---------------------------
X_test <- testData_clean[, c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
                             "Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate", "Age")]

# Predict using the random forest model trained on the training dataset
predictions_test <- predict(rf_model, newdata = X_test)

# Load the actual target values for the testData_clean dataset
y_test <- testData_clean$Strength

# Calculate Mean Absolute Error (MAE)
mae_test <- mean(abs(predictions_test - y_test))
cat("Mean Absolute Error (MAE) on test data:", mae_test, "\n")

# Calculate Mean Squared Error (MSE)
mse_test <- mean((predictions_test - y_test)^2)
cat("Mean Squared Error (MSE) on test data:", mse_test, "\n")

# Calculate Root Mean Squared Error (RMSE)
rmse_test <- sqrt(mse_test)
cat("Root Mean Squared Error (RMSE) on test data:", rmse_test, "\n")

# Calculate R-squared (R2) value
r_squared_test <- cor(predictions_test, y_test)^2
cat("R-squared (R2) value on test data:", r_squared_test, "\n")

# Function to classify concrete strength based on thresholds
classify_strength <- function(strength) {
  ifelse(strength < 17, "Low", 
         ifelse(strength <= 34, "Medium", "High"))
}

# Classify predictions and actual values
predicted_class <- classify_strength(predictions_test)
actual_class <- classify_strength(y_test)

# Confusion matrix
confusion_matrix <- table(predicted_class, actual_class)
print(confusion_matrix)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")

# Calculate sensitivity for each class
sensitivity_low <- confusion_matrix["Low", "Low"] / sum(confusion_matrix["Low", ])
sensitivity_medium <- confusion_matrix["Medium", "Medium"] / sum(confusion_matrix["Medium", ])
sensitivity_high <- confusion_matrix["High", "High"] / sum(confusion_matrix["High", ])
cat("Sensitivity (Low):", sensitivity_low, "\n")
cat("Sensitivity (Medium):", sensitivity_medium, "\n")
cat("Sensitivity (High):", sensitivity_high, "\n")

# Calculate specificity for each class
specificity_low <- (confusion_matrix["Medium", "Medium"] + confusion_matrix["High", "High"]) /
  (sum(confusion_matrix["Medium", ]) + sum(confusion_matrix["High", ]))
specificity_medium <- (confusion_matrix["Low", "Low"] + confusion_matrix["High", "High"]) /
  (sum(confusion_matrix["Low", ]) + sum(confusion_matrix["High", ]))
specificity_high <- (confusion_matrix["Low", "Low"] + confusion_matrix["Medium", "Medium"]) /
  (sum(confusion_matrix["Low", ]) + sum(confusion_matrix["Medium", ]))
cat("Specificity (Low):", specificity_low, "\n")
cat("Specificity (Medium):", specificity_medium, "\n")
cat("Specificity (High):", specificity_high, "\n")

# False positive and false negative counts
fp_count <- sum(confusion_matrix["Medium", "Low"])
fn_count <- sum(confusion_matrix["Low", "Medium"])
cat("False Positives:", fp_count, "\n")
cat("False Negatives:", fn_count, "\n")

# Calculate Kappa
total_agreement <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
expected_agreement <- sum(rowSums(confusion_matrix) * colSums(confusion_matrix)) / sum(confusion_matrix)^2
kappa <- (total_agreement - expected_agreement) / (1 - expected_agreement)
cat("Kappa:", kappa, "\n")

# ROC curve and AUC
library(pROC)
# Convert predicted and actual classes to binary (Low vs. Not Low)
binary_predicted <- ifelse(predicted_class == "Low", 1, 0)
binary_actual <- ifelse(actual_class == "Low", 1, 0)

# Calculate ROC curve and AUC
roc_obj <- roc(binary_actual, binary_predicted)
auc <- auc(roc_obj)
cat("AUC:", auc, "\n")
plot(roc_obj, print.auc = TRUE, main = "ROC Curve")

#-----------------------Extreme Gradient Boosting Model trainging
# Install and load necessary packages
library(xgboost)

# Load the trainData_clean dataset (assuming it's already loaded or imported)
# Replace 'trainData_clean' with the actual name of your dataset
data <- trainData_clean

# Assuming you know the column names of your target variable and predictor variables
target_column_name <- "Strength"  # Replace 'target_column' with the name of your target column
predictor_column_names <- c("Cement", "Blast.Furnace.Slag", "Fly.Ash", "Water",
                            "Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
                            "Age")  # Exclude the target column

# Separate the predictors (features) and the target variable
X <- data[, predictor_column_names]
y <- data[, target_column_name]

# Set up k-fold cross-validation
nfolds <- 10  # Number of folds for cross-validation

# Train the XGBoost model using k-fold cross-validation
xgb_cv <- xgb.cv(data = as.matrix(X), label = y, nfold = nfolds, nrounds = 100, objective = "reg:squarederror")

# Extract the optimal number of boosting rounds based on cross-validation
optimal_nrounds <- which.min(xgb_cv$evaluation_log$test_rmse_mean)

# Train the final XGBoost model with the optimal number of boosting rounds
xgb_model <- xgboost(data = as.matrix(X), label = y, nrounds = optimal_nrounds, objective = "reg:squarederror")

# Make predictions on the testing set
predictions <- predict(xgb_model, as.matrix(X))

# Calculate R-squared (R2)
r2 <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)

# Calculate adjusted R-squared (adjusted R2)
n <- nrow(X)
p <- ncol(X)
adjusted_r2 <- 1 - (1 - r2) * ((n - 1) / (n - p - 1))

# Calculate mean squared error (MSE)
mse <- mean((predictions - y)^2)

# Calculate mean absolute error (MAE)
mae <- mean(abs(predictions - y))

# Print the evaluation metrics
print(paste("RMSE:", rmse))
print(paste("R-squared (R2):", r2))
print(paste("Adjusted R-squared:", adjusted_r2))
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Mean Absolute Error (MAE):", mae))

#----------------------------EGB on test dataset--------------------
# Load the testData_clean dataset (assuming it's already loaded or imported)
# Replace 'testData_clean' with the actual name of your test dataset
test_data <- testData_clean

# Assuming you know the column names of your predictor variables in the test dataset
# If the test dataset includes the target variable, you may need to remove it
test_X <- test_data[, predictor_column_names]

# Make predictions on the test set
test_predictions <- predict(xgb_model, as.matrix(test_X))

# Calculate R-squared (R2) on the test set
test_r2 <- 1 - sum((test_data[, target_column_name] - test_predictions)^2) / sum((test_data[, target_column_name] - mean(test_data[, target_column_name]))^2)

# Calculate adjusted R-squared (adjusted R2) on the test set
test_n <- nrow(test_X)
test_p <- ncol(test_X)
test_adjusted_r2 <- 1 - (1 - test_r2) * ((test_n - 1) / (test_n - test_p - 1))

# Calculate mean squared error (MSE) on the test set
test_mse <- mean((test_predictions - test_data[, target_column_name])^2)

# Calculate mean absolute error (MAE) on the test set
test_mae <- mean(abs(test_predictions - test_data[, target_column_name]))

# Print the evaluation metrics for the test set
print(paste("Test R-squared (R2):", test_r2))
print(paste("Test Adjusted R-squared:", test_adjusted_r2))
print(paste("Test Mean Squared Error (MSE):", test_mse))
print(paste("Test Mean Absolute Error (MAE):", test_mae))
